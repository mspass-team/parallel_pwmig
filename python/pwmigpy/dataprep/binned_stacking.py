#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This module contains functions used to implement a CLI tool called 
"pseudosource_stacker".   That tool is a genarlization of an older 
C++ program called RFeventstacker.  The key concept is stacking of all 
data assmebled into bins using some document key to define the grouping.  
The functions I am creating initially duplicate the functionality of 
RFeventstacker, which is groups of sources that are geographcally "close".   
Note, however, that the general concept could be applied to something 
as different as CMP stacking.   For global seismology a better example 
would be core-mantle boundary reflection studies with data grouped by 
the geographic location of ScS or PcP midpoints.

Created on Wed Sep 24 07:30:05 2025

@author: pavlis
"""

from mspasspy.util.Undertaker import Undertaker
from mspasspy.ccore.utility import ErrorSeverity,Metadata
from mspasspy.ccore.seismic import (SeismogramEnsemble,
                                    TimeSeries,
                                    Seismogram,
                                    )

from mspasspy.algorithms.basic import ExtractComponent
from mspasspy.algorithms.MCXcorStacking import robust_stack
from mspasspy.util.seismic import number_live

def load_and_sort(db,
                  key_or_query_list,
                  query_key="source_id",
                  sort_key="site_id",
                  collection="wf_Seismogram",
                  data_type="Seismogram",
                  drop_abortions=True,
                  )->dict:
    """
    Load data defined by a list of key values and sort them into 
    a list of ensembles using a different key.
    
    Data are commonly stored in a particular sort order that is natural 
    for the algorithm or data collection system from which they were derived.
    e.g. reflection data is always collected as common source gathers.   
    Similarly, data downloaded from the FDSN is commonly loaded as 
    common source gathers driven by earthquake location data.   A downstream
    processing algorithm, however, often needs the data in a completely 
    different order.   The case in point that led me to create this generic 
    function in the first place was converted wave imaging where the 
    deconvolved output data is naturally organized by source but many 
    algorithms need the data organized by station.  In particular, for 
    pwmig this is needed to create composite source solutions using 
    groupings defined by the telecluster program.   The impulse response 
    estimates from the source groups are vertically stacked for each station
    to produce composite sources for an entire large array using that 
    algorithm.
    
    It is IMPORTANT to understand that this algorithm is necessary for 
    efficiency only when working with files and a large data set.  It is 
    known to be especially important for data stored on a large disk array 
    using Lustre where there is a large cost for opening files.   Running 
    on a cloud system like AWS S3 would be subject to an even worse overhead.
    What this function does is read data in what it assumes is the 
    natural order defined by the list of ids passed as arg1.  That is, 
    the outer loop of the algorithm is a loop over the list passed as 
    arg1 with a query or an implicit query with a list of object ids 
    that are assumed actual values of the key defined by `query_key`.  
    In that loop each ensemble is read and the data are sorted 
    in memory into slots keyed by the `sortkey` value.   Any inputs 
    that do not have a value for the `sortkey` will be killed 
    and buried with an `Undertaker` object creaed inside this function.
    Component 0 of the return will be a dictionary with keys defined by 
    actual values found for `sortkey` and an ensemble of data with 
    that value as the value of `sortkey`.  
    
    :param db:  MongoDB or MsPASS instance of a Database object
    :key_or_query_list: Must be a list.  That list must contain 
      one of two types of data:  (a) items that can define a unique match
      that define initial grouping (normally ObjectIds), or (b) python 
      dictionaries that are assumed to be valid MongoDB query operators.  
      When the content is dictionaries they are used in the outermost 
      loop to run a find followed by read_data to load ensembles.  
      When passed as a list of ObjectIds, a list of queries is 
      generated internally using equality matching on the key value 
      defined by the `query_key` argument.
    :param query_key:   key name to use if the list defined the 
      required argument `key_or_query_list` argument is not a list of 
      dictionaries.   In that case, the queries are generated by equality 
      matching againts this key name and values defined by the content of 
      the arg1 list.  Normally that is ObjectIds, but anything that can 
      guarantee a unique match would work.  This argument is ignored if 
      `key_or_query_list` is a list of dictionaries.  
    :type query_key:  string (default is "source_id")
    :param sort_key:   key by which the data ahould be sorted for output.  
    :type sort_key:   string (default "site_id")
    :param collection:   MongoDB collection name containing waveform 
      metadata documents that are the target of queries.   The function 
      dogmatically demands that if data_type is "TimeSeries" collection 
      must be either "wf_TimeSeries" or "wf_miniseed".  If data_type is
      "Seismogram" it requires collection to be "wf_Seismogram".   
      The function is dogmatic about that requirement as it would be 
      easy for even an experienced user to mix that up.  If additional 
      wf collections are added in the future more flexibility could be 
      easily added here.
    :type collection:  string (default "wf_Seismogram")
    :param data_type:   Expected data type of atomic data linked to each 
      document retrieved from `collection`.   
    :type data_type: string (default "Seismogram").  Must be either 
      "Seismogram" or "TimeSeries".  If not, the function will throw a 
      ValueError exception.
    :return:  dictionary of ensembles keyed by sort_key values.
    """
    # first verify argumens and consistency
    # if db is wrong assume exceptions will be thrown that make that 
    # issue clear
    prog = "load_and_sort"
    if not isinstance(key_or_query_list,list):
        message = prog + ":  illegal type for arg1={}\n".type(key_or_query_list)
        message += "Must be a list of key values or python dictionaries"
        raise ValueError(message)
    # assume all list elements are the same
    testval = key_or_query_list[0]
    if isinstance(testval,dict):
        query_list = key_or_query_list
    else:
        query_list=list()
        for val in key_or_query_list:
            query={query_key : val}
            query_list.append(query)
    # be dogmatic about collection name
    # this may be unnecesary but the errors trapped here would be very 
    # easy to commit
    if data_type == "TimeSeries":
        if collection not in ["wf_TimeSeries","wf_miniseed"]:
            message = prog + ":  Illegal arg combination\n"
            message += "data_type is TimeSeries but collection={}\n".format(collection)
            message += "for TimeSeries data collection must be either wf_TimeSeries or wf_miniseed"
            raise ValueError(message)
    elif data_type == "Seismogram":
        if collection != "wf_Seismogram":
            message = prog + ":  Illegal arg combination\n"
            message += "data_type is Seismogram but collection={}\n".format(collection)
            message += "for Seismogram data collection must be wf_Seismogram"
            raise ValueError(message)
    else:
        message = prog + ":  data_type={} is not allowed.  Must be either TimeSeries or Seismogram".format(data_type)
        raise ValueError(message)
    
    # we bury abortions with ths undertaker
    stedronsky = Undertaker(db)
    dbcol = db[collection]
    sorted_data = dict()
    for query in query_list:
        cursor = dbcol.find(query)
        e = db.read_data(cursor,collection=collection)
        # note most dictionaries use string keys but docuentation says 
        # any immutable type can serve as a dictionary key.   The most 
        # common key to use here will be ObjectId values which will work
        # in this context
        for d in e.member:
            if d.dead():
                stedronsky.bury(d)
            else:
                if sort_key in d:
                    this_key=d[sort_key]
                    if this_key in sorted_data:
                        sorted_data[this_key].append(d)
                    else:
                        sorted_data[this_key] = [d]
                else:
                    message = "sort_key={} not found for this datum so processing by this algorithm failed".format(sort_key)
                    d.elog.log_error(prog,message,ErrorSeverity.Invalid)
                    d.kill()
                    stedronsky.bury(d)
    return sorted_data


def linear_stack(ens,weight_key=None,undefined_weight=0.0):
    """
    Linear stack means simple summation averaging.  optional weighting using 
    weight_key value extracted from Metadaa of each member
    
    """
    sumwt = 0.0
    result=None
    for i in len(ens.member):
        d = ens.member[i]
        if d.live:
            if weight_key:
                if weight_key in d:
                    wt = d[weight_key]
                else:
                    wt = undefined_weight
                if wt>0.0:
                    d.data *= wt
                sumwt += wt
            else:
                sumwt += 1.0
            if result is None:
                # this needs to be a copy
                result = Seismogram(d)
            else:
                result += d
    # handle null result
    if result is None or sumwt<=0.0:
        # awkward but necessary to return an appropriate null result 
        # return a default constructed atomic datum if all failed
        if isinstance(ens.TimeSeriesEnsemble):
            return TimeSeries()
        elif isinstance(ens,SeismogramEnsemble):
            return Seismogram()
        else:
            # this exception should never be thrown as above will 
            # abort before this but this is a safetyy value
            raise ValueError("linear_stack:  received invalid data for arg0")
    result.data /= sumwt
    return result
            
                
def magnitude_power_weight(d,
                      key="magnitude",
                      scale=3.0,
                      power=3.0,
                      default=10.0,
                      ceiling=100.0,
                      origin=4.0,
                      )->float:
    """
    Returns magnitude-dependent weight.  
    
    All single station receiver function estimates in MsPASS are 
    normalized to have amplitudes independent of magnitude.  Increasing 
    magntidue, however, by definition implies larger ampitudes which 
    translates to more conficence in teh estimates for larger 
    magnitudes.  This function can be used tioimplement q weighting 
    scheme to increase teh weight of larger magnitude events.  
    
    This implementation support only an experimental power law weight function that 
    depends on magnitude extracted from the Metadata conainer of d.  
    The formula is:
        weight = [scale*(magnitude - origin)]^power
    The maximum allowed weight is defined by the `ceiling` argument. 
    That is, for large magnitude events when the computed weight exceeds 
    the ceiling the return will be the ceiling value.   Note
    variations in power, scale, origin, and celing can be used to 
    produce flat weights for large magnitudes with power law 
    taper passing through 1.0 at origin.  On the other extreme, 
    setting power to 10.0, scale=1.0, and ceiling=large value creates an approximate 
    true amplitude normalization if the data are standard receiver 
    functions that have normalized amplitudes that are independet of 
    magnitudes.  A range of cutoff behavior and form can be achieved
    by varying the scale, ceiling and power values.   I recommend 
    creating a graphic of the weights this will use as a function of 
    magnitude if you use anything but the default.  
    
    The function depends on a magntidue estimate being set in the 
    Metadaa container of d.  For data with a mix of magnitude estimaes 
    with different keys caller will need program logic to handle the 
    variations.   Teh default is a generic "magnitude" key which is 
    the norm for data from FDSN retrieved with quakeML.  
    """
    if d.dead() or key not in d:
        return default
    mag = d[key]
    dm = mag - origin
    wt = pow(scale*dm,power)
    if wt>ceiling:
        wt = ceiling
    return wt

def robust_stack_3C(ensemble,
                    method="dbxcor",
                    stack0=None,
                    stack_md=None,
                    timespan_method="ensemble_inner",
                    pad_fraction_cutoff=0.05,
                    residual_norm_floor=0.01,
                ):
    """
    Applied MCXcor robust_stack function for each of three components 
    and then merges the three TimeSeries to return a Seismogram 
    from all three components.   The args here are identical to 
    to MCXcrStacking.robust_stack.
    
    The function returns a tuple that is an extension of that 
    returned by the MXcorStacking.robust_stack function.  
    That is, like that function component 0 is the robust stack.  
    The oddities is component 1.  For scalar input the return is a
    vector of weights.   In this case the return is a python list 
    containing three numpy vectors containing the weights used for 
    each component.   That can be useful for expermental alternatives 
    for handling the stack.  That is, this function computes weighs 
    independently for each component, but one might choose to 
    normalize the weights to a single weight for each Seismogram 
    to avoid relative amplitude distortion.  
    """
    # toDO  make sure ensemble is a SeismgoramEnsemble
    stacklist=list()
    wtlist=list()
    for i in range(3):
        ets = ExtractComponent(ensemble,i)
        rsout = robust_stack(ets,
                             method,
                             stack0,
                             stack_md,
                             timespan_method,
                             pad_fraction_cutoff,
                             residual_norm_floor)
        stacklist.append(rsout[0])
        wtlist.append(rsout[1])
    # we can assume all the components yield the same length output 
    # so we just clone component 0.  With the Metadata constructor 
    # the size will be set by the Metadata content.
    md = Metadata(stacklist[0])
    # this special constructor clones md will initialize the data matrix
    # to the size defined by  the TimeSeries stacklist[0] from which md was 
    # extracted
    stack = Seismogram(md,False)
    for i in range(3):
        stack.data[i,:] = stacklist[i].data
    stack.set_live()
    return [stack,wtlist]
    
        
    
def stack_groups(keyed_ensemble,
                 method="median",
                 weight_key=None,
                 undefined_weight=0.0,
                 stack0=None,
                 stack_md=None,
                 timespan_method="ensemble_inner",
                 pad_fraction_cutoff=0.05,
                 residual_norm_floor=0.01,
                 )->SeismogramEnsemble:
    """
    Top-level function used to stack output of data created by load_and_stack 
    function in this module.
    
    This function was created primarily for stacking impulse response 
    estimates grouped by region with the CLI tool telecluster.   I tried to 
    make it generic enough, however, that it should prove useful for a 
    range of research problems where the data are binned by some external 
    algorithm into groups that can be vertically stacked (i.e. averaged 
    without a time shift).  
    
    The algorithm stacks contents of a dictioary where the keys of the 
    dictionary are the unique key used for sorting the data into 
    the groups that are the values associated with that key.  The groups
    are stored as SeismogramEnsemble objects - i.e. the value associated 
    with each key is assumed to be a SeismogramEnsemble.  
    
    Currently four algorithms can be selected via the method argument:
        "average" - compute a simple arithmetic mean as the stack.  
           For the record that is the standard way seismic reflection 
           data are "stacked".  All data are given equal weight in 
           the stack
         "weighted_stack" - invokes a weighted mean with the weights 
           extracted from each atomic datum's Metatdata container 
           with the (then required) key defined by the "weight_key"
           argument.  Note if this method is selected "weight_key" 
           must be defined the function will abort by throwning a 
           ValueError exception.
         "median" - invokes a median stack.  This behaves exactly like 
           average except the measure of center for each output sample is 
           the median instead of the mean.   Sometimes also called an L1 stack.
         "robust_dbxcor" - invoke the robust stack method used in dbxcor 
           with the mspass implementation in the spasspy.algorithms.MCXcorStacking
           module.   This argument uses the parameters with the same 
           name in the robust_stack function with these arg names in this 
           function:  stack0, stack_md, timespan_method, pad_fraction_cutoff,
           and residual_norm_floor.   See the docstring for 
           mspsspy.algorithms.MCXdorStacking.robust_stack for guidance.
           
    """
    # check args
    # this type test is a sanity check only.  For now assume the 
    # dictionary content is ok if this resolves
    prog = "stack_groups"
    if not isinstance(keyed_ensemble,dict):
        message = prog
        message += ":  arg0 must be a dictionary with SeismogramEnsemble values\n"
        message += "Actual type of arg0 is {}".format(type(keyed_ensemble))
        raise ValueError(message)
    # make sure this list is consistent with the match-case elements 
    # in the algorithm below
    allowed_methods = ["average","weighted_average","median","robust_dbxcor"]
    if method not in allowed_methods:
        message = prog
        message += ":  method={} is supported.  Must be one of:  {}".format(method,allowed_methods)
        raise ValueError(message)
    if method == "weighted_average" and weight_key is None:
        message = prog
        message += ":  illegal argument combination.\n"
        message += "method is set to weighted_stack but weight_key value was no specified.\n"
        message += "Tha algorithm needs to fetch weights from Metadata using the key defined by the weight_key argument"
        raise ValueError(message)
    
    # note this constructor sets u slos for members but doesn't 
    # doesn't create any Seismogram objects
    stacked_data = SeismogramEnsemble(len(keyed_ensemble))
    # This echos args for this function as a subdocument posted to the 
    # output's Metadata container
    argdoc=dict()
    argdoc["method"]=method
    if method=="weighted_stack":
        argdoc["weight_key"] = weight_key
        argdoc["undefined_weight"] = undefined_weight
    if method in ["median","robust_dbxcor"]:
        if stack_md:
            argdoc["stack_md"] = stack_md
        argdoc["timespan_method"] = timespan_method
        argdoc["pad_fraction_cutoff"] = pad_fraction_cutoff
        argdoc["residual_norm_floor"] = residual_norm_floor
    stacked_data["stacker_arguments"]=argdoc
    for ekey in keyed_ensemble:
        ensemble = keyed_ensemble[ekey]
        
        # old versions of python will fail on this line but since this 
        # construct was added after 3.10 I will use it here for improved 
        # raadability and efficiency
        match method:
            case "average":
                stack = linear_stack(ensemble)
            case "weighted_average":
                stack = linear_stack(ensemble,
                                     weight_key=weight_key,
                                     undefined_weight=undefined_weight)
            case "median":
                stack,wts = robust_stack_3C(ensemble,
                                        method="median",
                                        stack0=None,
                                        stack_md=stack_md,
                                        timespan_method=timespan_method,
                                        pad_fraction_cutoff=pad_fraction_cutoff,
                                        residual_norm_floor=residual_norm_floor,
                                    )
            
            case "robust_dbxcor":
                # for present discard wts component weights
                # possible extension is restack using something like max or 
                # min weights as a single weight for each Seismogram
                # this handles each component independently
                stack,wts = robust_stack_3C(ensemble,
                                        method="dbxcor",
                                        stack0=None,
                                        stack_md=stack_md,
                                        timespan_method=timespan_method,
                                        pad_fraction_cutoff=pad_fraction_cutoff,
                                        residual_norm_floor=residual_norm_floor,
                                    )
            case _:
                message = prog
                message += ":  landed in illegal method case of processing loop"
                message += "This should not happen and is a fatal error\n"
                message += "Likely error in code defining supported methods list"
                raise RuntimeError(message)
        # append output even if it is marked dead
        stack["stack_sort_key_value"] = ekey
        stacked_data.member.append(stack)
    if number_live(stacked_data)>0:
        stacked_data.set_live()
    return stacked_data
    
        
    
    

